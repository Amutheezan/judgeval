{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "In this notebook, we cover how to implement a `PromptScorer`. To guide our example, let's imagine we have a customer support chatbot and want to evaluate whether its responses are polite/positive.\n",
    "\n",
    "`PromptScorers` are powerful LLM-based scorers that are analogous to [LLM Judges](https://arxiv.org/abs/2306.05685). You can use Judgment to create custom LLM judges that are best suited to your specific evaluation case! Before you try implementing an LLM judge, you should check if any ready-made Judgment scorers already fit your evaluation needs.\n",
    "\n",
    "With that, let's break down the `PromptScorer` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joe/Desktop/Work/judgeval\n",
      "Pipfile       README.md     \u001b[1m\u001b[36mjudgeval\u001b[m\u001b[m/     main.py\n",
      "Pipfile.lock  \u001b[1m\u001b[36mdocs\u001b[m\u001b[m/         \u001b[1m\u001b[36mlogs\u001b[m\u001b[m/         \u001b[1m\u001b[36mtests\u001b[m\u001b[m/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joe/.local/share/virtualenvs/judgeval-JrQ-jo5P/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (run_evaluation.py, line 169)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/.local/share/virtualenvs/judgeval-JrQ-jo5P/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[6], line 8\u001b[0m\n    from judgeval.judgment_client import JudgmentClient\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Desktop/Work/judgeval/judgeval/judgment_client.py:13\u001b[0;36m\n\u001b[0;31m    from judgeval.run_evaluation import run_eval\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Desktop/Work/judgeval/judgeval/run_evaluation.py:169\u001b[0;36m\u001b[0m\n\u001b[0;31m    debug(\"Sending request to Judgment API\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"./judgeval\")  # root of judgeval\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from judgeval.judgment_client import JudgmentClient\n",
    "from judgeval.data import Example\n",
    "from judgeval.judges import TogetherJudge\n",
    "from judgeval.scorers import PromptScorer\n",
    "\n",
    "\n",
    "qwen = TogetherJudge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building our custom prompt scorer**\n",
    "\n",
    "Every prompt scorer that inherits from the `PromptScorer` class must implement the `build_measure_prompt()` class method. This method takes an `Example` and creates a prompt for the LLM judge based on the data. The only constraint is that the prompt must dictate that the judge produce a JSON in its answer with two fields: `score` and `reason`. These can be used in our `check_success()` method later!\n",
    "\n",
    "Since we're trying to evaluate the sentiment of our chatbot's responses, let's have our judge examine a question and the answer produced by our chatbot. Then the judge will determine whether the chatbot's response was positive or negative.\n",
    "\n",
    "Lastly, we must implment the `check_success()` class method. This method determines whether a single `Example` is successful if treated as a test case. In our case, we want our chatbot to respond with neutral or positive sentiment (never negative!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentScorer(PromptScorer):\n",
    "    \"\"\"\n",
    "    Detects negative sentiment (angry, sad, upset, etc.) in a response\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        name=\"Sentiment Scorer\", \n",
    "        threshold=0.5, \n",
    "        model=qwen, \n",
    "        include_reason=True, \n",
    "        async_mode=True, \n",
    "        strict_mode=False, \n",
    "        verbose_mode=False\n",
    "        ):\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            threshold=threshold,\n",
    "            model=model,\n",
    "            include_reason=include_reason,\n",
    "            async_mode=async_mode,\n",
    "            strict_mode=strict_mode,\n",
    "            verbose_mode=verbose_mode,\n",
    "        )\n",
    "        self.score = 0.0\n",
    "\n",
    "    def build_measure_prompt(self, example: Example):\n",
    "        SYSTEM_ROLE = (\n",
    "            'You are a great judge of emotional intelligence. You understand the feelings ' \n",
    "            'and intentions of others. You will be tasked with judging whether the following '\n",
    "            'response is negative (sad, angry, upset) or not. After deciding whether the '\n",
    "            'response is negative or not, you will be asked to provide a brief, 1 sentence-long reason for your decision.'\n",
    "            'You should score the response based on a 1 to 5 scale, where 1 is not negative and '\n",
    "            '5 is very negative. Please end your response in the following JSON format: {\"score\": <score>, \"reason\": <reason>}'\n",
    "                  )\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_ROLE},\n",
    "            {\"role\": \"user\", \"content\": f\"Response: {example.actual_output}\\n\\nYour judgment: \"}\n",
    "        ] \n",
    "\n",
    "    def success_check(self):\n",
    "        POSITIVITY_THRESHOLD = 3  # we want all model responses to be somewhat positive in tone\n",
    "        return self.score <= POSITIVITY_THRESHOLD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying out our scorer**\n",
    "\n",
    "That's it! We can now run our prompt scorer on some examples and see how it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized JudgmentClient, welcome back Joseph Camyre!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m scorer \u001b[38;5;241m=\u001b[39m SentimentScorer()\n\u001b[1;32m      8\u001b[0m client \u001b[38;5;241m=\u001b[39m JudgmentClient()\n\u001b[0;32m----> 9\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos_example\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQWEN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/Desktop/judgment_labs/judgeval/judgeval/judgment_client.py:48\u001b[0m, in \u001b[0;36mJudgmentClient.run_evaluation\u001b[0;34m(self, examples, scorers, model, aggregator, metadata)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03mExecutes an evaluation of `Example`s using one or more `Scorer`s\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28meval\u001b[39m \u001b[38;5;241m=\u001b[39m EvaluationRun(\n\u001b[1;32m     41\u001b[0m     examples\u001b[38;5;241m=\u001b[39mexamples,\n\u001b[1;32m     42\u001b[0m     scorers\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     judgment_api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjudgment_api_key\n\u001b[1;32m     47\u001b[0m )\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/judgment_labs/judgeval/judgeval/run_evaluation.py:149\u001b[0m, in \u001b[0;36mrun_eval\u001b[0;34m(evaluation_run)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Run local evals\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_scorers:  \u001b[38;5;66;03m# List[CustomScorer]\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     results: List[ScoringResult] \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43ma_execute_scoring\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevaluation_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_scorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_on_missing_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshow_indicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_use_bar_indicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mthrottle_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_concurrent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     local_results \u001b[38;5;241m=\u001b[39m results\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# TODO: Once we add logging (pushing eval results to Judgment backend server), we can charge for # of logs\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Pass in the API key to these log requests.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# for result in results:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Aggregate the ScorerData from the API and local evaluations\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "pos_example = Example(\n",
    "    input=\"What's the store return policy?\",\n",
    "    actual_output=\"Our return policy is wonderful! You may return any item within 30 days of purchase for a full refund.\",\n",
    ")\n",
    "\n",
    "scorer = SentimentScorer()\n",
    "\n",
    "client = JudgmentClient()\n",
    "results = client.run_evaluation(\n",
    "    [pos_example],\n",
    "    [scorer],\n",
    "    model=\"QWEN\"\n",
    ") \n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**\n",
    "\n",
    "Now that we've seen how to implement a prompt scorer, try adapting it to your use case! Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
