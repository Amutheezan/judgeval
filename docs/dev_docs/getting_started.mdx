# Quick Introduction

Judgeval is an evaluation framework for LLM systems. Judgeval is designed for AI teams to build on and iterate on their LLM systems (applications) and was built to
- Easily unit test LLM systems.
- Supply a quality control layer for multi-step LLM applications, especially for **agentic systems**.
- Plug-and-evaluate LLM systems with 10+ research-backed metrics including hallucination detection, RAG retrieval quality, and more.
- Construct custom evaluation pipelines for your LLM systems.
- Monitor LLM systems in production using state-of-the-art real-time evaluation foundation models.


Additionally, Judgeval integrates natively with Judgment Labs, allowing you to evaluate, regression test, and monitor LLM applications in the cloud.

Judgeval was built from a team of LLM researchers from Stanford, Datadog, and Together AI.

# Installation 

`pip install judgeval`

Judgeval runs evaluations on your local machine. However, you may find it easier to directly run evaluations using Judgment Labs' infrastructure, an all-in-one platform for LLM system evaluation.

# Making a Judgment Key

TODO


Tip: Running evaluations on Judgment Labs' infrastructure is recommended for large-scale evaluations. Contact us if you're dealing with sensitive data that has to reside in your private VPCs.

# Create your first evaluation


```
from judgeval.data import Example
from judgeval.scorers import JudgmentScorer
from judgeval.judgment_client import JudgmentClient
from judgeval.constants import APIScorer

client = JudgmentClient()

example = Example(
    input="What if these shoes don't fit?",
    actual_output="We offer a 30-day full refund at no extra cost.",
    retrieval_context=["All customers are eligible for a 30 day full refund at no extra cost."],
)

scorer = JudgmentScorer(threshold=0.5, score_type=APIScorer.FAITHFULNESS)
results = client.run_evaluation(
    examples=[example],
    scorers=[scorer],
    model="gpt-4o",
)
print(results)
```

Congratulations! Your evaluation should have passed. Let's break down what happened.

- The variable `input` mimics a user input and `actual_output` is a placeholder for what your LLM system returns based on the input.
- The variable `retrieval_context` represents the retrieved context from your knowledge base and `JudgmentScorer(threshold=0.5, score_type=APIScorer.FAITHFULNESS)` is a scorer that checks if the output is hallucinated relative to the retrieved context.
- Scorers give values betweeen 0 - 1 and we set the threshold for this scorer to 0.5 in the context of a unit test. If you are interested measuring rather than testing, you can ignore this threshold and reference the `score` field alone.
- We chose `gpt-4o` as our judge model for faithfulness. Judgment Labs offers ANY judge model for your evaluation needs.

# Create Your First Scorer
`judgeval` offers three kinds of LLM scorers for your evaluation needs: ready-made, prompt scorers, and custom scorers.

## Ready-made Scorers
Judgment Labs provides default implementations of 10+ research-backed metrics covering evaluation needs ranging from hallucination detection to RAG retrieval quality. To create a ready-made scorer, just import it directly from `judgeval.scorers`:

```
from judgeval.judgment_client import JudgmentClient
from judgeval.data import Example
from judgeval.scorers import JudgmentScorer
from judgeval.constants import APIScorer

client = JudgmentClient()
example = Example(
    input="...",
    actual_output="...",
    retrieval_context=["..."],
)
scorer = JudgmentScorer(threshold=0.5, score_type=APIScorer.FAITHFULNESS)

results = client.run_evaluation(
    examples=[example],
    scorers=[scorer],
    model="gpt-4o",
)
print(results)
```

## Prompt Scorers
`judgeval` allows you to create custom scorers using natural language. These can range from simple judges to powerful evaluators for your LLM systems.

```
TODO
```

## Custom Scorers
If you find that none of the ready-made scorers or prompt scorers fit your needs, you can create your own custom scorer. These can be as simple or complex as you need them to be and do not have to use an LLM judge model. Here's an example of computing BLEU scores:

```
import sacrebleu
from judgeval.scorers import CustomScorer 

class BLEUScorer(CustomScorer):
    def __init__(self, threshold: float = 0.5):
        super().__init__(score_type="BLEU", threshold=threshold)

    def score_example(self, example: Example) -> float:
        reference = example.expected_output
        candidate = example.actual_output

        score = sacrebleu.sentence_bleu(candidate, [reference]).score
        self.score = score
        return score

    # Async implementation of score_example(). If you have no async logic, you can
    # just use the synchronous implementation.
    async def a_score_example(self, example: Example) -> float:
        return self.score_example(example)

    def success_check(self) -> bool:
        return self.score >= self.threshold

    @property
    def __name__(self):
        return "BLEU"

# example usage 
example = Example("input"="...", "actual_output"="...", "expected_output"="...")
scorer = BLEUScorer()
results = scorer.score_example(example)
print(results)
```

## Running Multiple Scorers Simultaneously

If you're interested in measuring multiple metrics at once, you can group scorers together when running evaluations, regardless of the type of scorer.

```
from judgeval.judgment_client import JudgmentClient
from judgeval.scorers import JudgmentScorer
from judgeval.constants import APIScorer

client = JudgmentClient()

faithfulness_scorer = JudgmentScorer(threshold=0.5, score_type=APIScorer.FAITHFULNESS)
summarization_scorer = JudgmentScorer(threshold=0.8, score_type=APIScorer.SUMMARIZATION)

results = client.run_evaluation(
    examples=[example],
    scorers=[faithfulness_scorer, summarization_scorer],
    model="gpt-4o",
)
```

# Create Your First Dataset 
In most cases, you will not be running evaluations on a single example; instead, you will be scoring your LLM system on a dataset. `judgeval` allows you to create datasets, save them, and run evaluations on them. An `EvalDataset` is a collection of `Example`s and/or `GroundTruthExample`s.

`Note: A GroundTruthExample is an Example that has no actual_output field since it will be generated at test time.`

```
from judgeval.data import Example, GroundTruthExample, EvalDataset

example1 = Example("input"="...", "actual_output"="...")
example2 = Example("input"="...", "actual_output"="...")

dataset = EvalDataset(examples=[example1, example2])
```

Then, you can run evaluations on the dataset:

```
...

client = JudgmentClient()
scorer = JudgmentScorer(threshold=0.5, score_type=APIScorer.FAITHFULNESS)
results = client.evaluate_dataset(
    dataset=dataset,
    scorers=[scorer],
    model="QWEN",
)
```


# Using Judgment Labs Platform

## Managing Datasets 

## Creating PromptScorers

## Optimizing System Performance

## Monitoring LLM Systems in Production



