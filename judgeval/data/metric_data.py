"""
Implementation of the MetricData class.
"""

from typing import List, Union, Optional, Dict
from pydantic import BaseModel, Field

from judgeval.scorers.custom_scorer import CustomScorer

class ScorerData(BaseModel):
    """
    ScorerData holds the infromation related to a single Scorer evaluation run.

    For example, if running the Judgment Faithfulness scorer on an example, the ScorerData
    object will contain whether the example passed its threshold expectation, as well as more detailed
    information surrounding the evaluation run such as the claims and verdicts generated by the
    judge model(s).
    """
    name: str
    threshold: float
    success: bool
    score: Optional[float] = None
    reason: Optional[str] = None
    strict_mode: Optional[bool] = Field(False, alias="strictMode")
    evaluation_model: Union[List[str], str] = Field(None, alias="evaluationModel")
    error: Optional[str] = None
    evaluation_cost: Union[float, None] = Field(None, alias="evaluationCost")
    verbose_logs: Optional[str] = Field(None, alias="verboseLogs")
    additional_metadata: Optional[Dict] = Field(None, alias="additionalMetadata")


def create_scorer_data(scorer: CustomScorer) -> ScorerData:
    """
    After a `scorer` is run, it contains information about the example that was evaluated
    using the scorer. For example, after computing Faithfulness, the `scorer` object will contain
    whether the example passed its threshold, the score, the reason for score, etc.

    This function takes an executed `scorer` object and produces a ScorerData object that
    contains the output of the scorer run that can be exported to be logged as a part of
    the ScorerResult.
    """
    if scorer.error is not None:  # error occurred during eval run
        return ScorerData(
            name=scorer.__name__,
            threshold=scorer.threshold,
            score=None,
            reason=None,
            success=False,
            strictMode=scorer.strict_mode,
            evaluationModel=scorer.evaluation_model,
            error=scorer.error,
            evaluationCost=scorer.evaluation_cost,
            verboseLogs=scorer.verbose_logs,
        )
    else:  # standard execution, no error
        return ScorerData(
            name=scorer.__name__,
            score=scorer.score,
            threshold=scorer.threshold,
            reason=scorer.reason,
            success=scorer.success_check(),
            strictMode=scorer.strict_mode,
            evaluationModel=scorer.evaluation_model,
            error=None,
            evaluationCost=scorer.evaluation_cost,
            verboseLogs=scorer.verbose_logs,
            additionalMetadata=scorer.additional_metadata,
        )
